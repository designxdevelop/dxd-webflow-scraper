# Plan: API to Cloudflare Workers + Scraper Optimizations

## Part A: Migrate API to Cloudflare Workers

### Phase 0 — Storage Package: Dual-Runtime Support

The `@dxd/storage` package currently depends on `node:fs`, `node:path`, `node:stream`. The worker service (Railway) still needs these. The Workers API needs R2 bindings. Both must coexist.

**1. Add R2 storage adapter**
- `packages/storage/src/r2.ts` — new `R2Storage` class implementing `StorageAdapter`
- Uses native R2 bucket bindings (`R2Bucket.put()`, `.get()`, `.list()`, `.delete()`)
- No Node.js imports — pure Web APIs
- `createTempDir` / `moveToFinal` are not needed for the API (only the worker uses them) — throw or no-op

**2. Split package exports**
- `packages/storage/package.json` — add conditional exports:
  - `@dxd/storage/s3` → `./src/s3.ts` (Node-only, worker service)
  - `@dxd/storage/r2` → `./src/r2.ts` (Workers-only, API)
  - `@dxd/storage` → `./src/index.ts` (re-exports adapter interface + factory)
- No changes to `S3Storage` — worker service continues using it as-is

**3. Point worker's S3 config at R2's S3-compatible endpoint**
- Update worker env vars: `S3_ENDPOINT` → `https://<account_id>.r2.cloudflarestorage.com`
- Same bucket, accessed via S3 protocol (worker) and R2 bindings (API)
- This eliminates dual-write — single R2 bucket, two access methods

**Files:**
- Create: `packages/storage/src/r2.ts`
- Modify: `packages/storage/package.json`, `packages/storage/src/index.ts`

---

### Phase 1 — Extract App Factory + Workers Entry Point

**1. Extract Hono app from `index.ts` into `app.ts`**
- `apps/api/src/app.ts` — `createApp(deps)` factory function
- Takes dependencies (db, storage, redis) as params instead of importing singletons
- All routes, middleware, CORS config move here
- Returns typed Hono app

**2. Create Workers entry point**
- `apps/api/src/worker-entry.ts` — `export default { fetch }` pattern
- Instantiates deps from `env` bindings (Hyperdrive, R2, Upstash)
- Calls `createApp(deps)` and delegates to `app.fetch(request)`

**3. Keep Node entry point working**
- `apps/api/src/index.ts` — becomes thin wrapper calling `createApp(deps)` with `process.env`-based deps + `serve()` from `@hono/node-server`
- Both entry points share identical route logic

**4. Define env types**
- `apps/api/src/env.ts` — `Bindings` type (Hyperdrive, R2Bucket, Upstash secrets, auth secrets) and `AppEnv` type for Hono generics

**5. Context middleware**
- `apps/api/src/middleware/context.ts` — reads deps from Hono context, sets `db`, `storage`, `redis` on `c.var`
- All route handlers switch from `import { db }` to `c.get('db')`

**6. Wrangler config**
- `apps/api/wrangler.toml` — worker name, `nodejs_compat` flag, R2 binding, Hyperdrive binding, routes

**Files:**
- Create: `apps/api/src/app.ts`, `apps/api/src/worker-entry.ts`, `apps/api/src/env.ts`, `apps/api/src/middleware/context.ts`, `apps/api/wrangler.toml`
- Modify: `apps/api/src/index.ts`, `apps/api/package.json` (add `wrangler`, `@cloudflare/workers-types`, `@upstash/redis`)

---

### Phase 2 — Migrate Routes (sequential, one at a time)

**2.1 — Database client** (`apps/api/src/db/client.ts`)
- Remove singleton `db` export
- Export `createDbClient(connectionString)` factory
- In Workers: `c.env.HYPERDRIVE.connectionString` → `postgres()` → `drizzle()`
- `postgres.js` v3.4+ works in Workers with `nodejs_compat`

**2.2 — Replace `process.env` everywhere**
- Every file in `apps/api/src/` that reads `process.env` → use `c.env.*` or accept as param
- Key files: `auth/config.ts`, `db/client.ts`, `queue/client.ts`, all route files

**2.3 — Queue enqueue** (`apps/api/src/queue/client.ts`, `services/worker/src/http.ts`)
- Add a thin HTTP endpoint to the worker service: `POST /enqueue` (accepts `{siteId, crawlId}`, enqueues via BullMQ)
- Protected with a shared `WORKER_API_SECRET` header
- Workers API calls this endpoint via `fetch()` instead of touching Redis/BullMQ directly
- This completely avoids BullMQ protocol compatibility issues from Workers
- The cancel route (`crawls/:id/cancel`) just updates DB status — the worker already polls for cancellation
- **New file:** `services/worker/src/http.ts` — Hono server with `/enqueue` and `/cancel` endpoints
- **Modify:** `services/worker/src/index.ts` — start HTTP server alongside BullMQ worker

**2.4 — SSE route** (`apps/api/src/routes/sse.ts`)
- Replace ioredis pub/sub with Upstash Redis polling
- Worker continues publishing via `redis.publish()` on its ioredis connection
- Worker also writes events to a Redis Stream (`XADD crawl-events:{crawlId}`) — small change to `services/worker/src/processor.ts` `publishEvent()`
- Workers API uses `@upstash/redis` to `XREAD` from the stream every 1-2s
- Returns SSE via `ReadableStream` with periodic polling
- Redis Streams auto-trim with `MAXLEN ~1000`

**2.5 — Preview route** (`apps/api/src/routes/preview.ts`)
- Replace `storage.readFile()` (returns `Buffer`) with R2 `.get()` (returns `R2ObjectBody`)
- Replace `node:path.extname()` with simple string split for file extension
- For HTML/CSS/JSON: read as text from R2, apply existing rewrite functions, return as `Response`
- Large binary assets: stream directly from R2 body without buffering
- HTML rewriting stays as regex-based (existing functions work fine — they're pure string transforms, no Node deps)

**2.6 — Zip download** (`apps/api/src/routes/crawls.ts`)
- Replace `archiver` (Node-only) with `fflate` or `client-zip` (Workers-compatible)
- List R2 objects → stream each through zip → return as `Response`
- Pre-built zips (from worker) served directly from R2 (already works via `storage.readStream()`)
- Fallback on-demand zip uses streaming Web API

**2.7 — Auth** (`apps/api/src/auth/config.ts`)
- `@auth/core` + `@hono/auth-js` already Workers-compatible
- Move secrets from `process.env` to `c.env`
- DrizzleAdapter takes the context-provided DB client
- Update `AUTH_URL` to Workers domain

**Files changed per step:**
- 2.1: `apps/api/src/db/client.ts`
- 2.2: All files with `process.env`
- 2.3: Create `services/worker/src/http.ts`, modify `services/worker/src/index.ts`, modify `apps/api/src/queue/client.ts`
- 2.4: `apps/api/src/routes/sse.ts`, `services/worker/src/processor.ts` (add XADD alongside publish)
- 2.5: `apps/api/src/routes/preview.ts`
- 2.6: `apps/api/src/routes/crawls.ts`
- 2.7: `apps/api/src/auth/config.ts`, `apps/api/src/auth/middleware.ts`

---

### Phase 3 — Infrastructure Provisioning

Assumption from user: an R2 bucket is already in use today.

1. **Cloudflare R2 bucket** — verify existing bucket name/permissions and reuse it (no new bucket required)
2. **Cloudflare Hyperdrive** — create config pointing to Railway Postgres connection string
3. **Upstash Redis** — create instance; worker uses ioredis-compatible endpoint (TCP), API uses REST endpoint
4. **Wrangler secrets** — `UPSTASH_REDIS_REST_URL`, `UPSTASH_REDIS_REST_TOKEN`, `AUTH_SECRET`, `GITHUB_CLIENT_ID`, `GITHUB_CLIENT_SECRET`, `WORKER_SERVICE_URL`, `WORKER_API_SECRET`
5. **R2 data migration** — skip if worker is already writing to this R2 bucket; otherwise perform one-time `rclone sync` and then switch worker S3 endpoint to R2
6. **DNS** — point API domain to Workers route

---

### Phase 4 — Deploy + Cutover

1. Deploy to Workers via `wrangler deploy`
2. Test on staging domain against real Hyperdrive/R2/Upstash
3. DNS cutover to Workers
4. Keep Node API on Railway as hot standby for 48-72h
5. Tear down Node API after stabilization

---

## Part B: Scraper Optimizations

All changes in `packages/scraper/` and `services/worker/src/processor.ts`. The worker stays on Railway/Node.

### B1 — Resumable Crawls + Partial Progress on Timeout

**Problem:** The state manager (`state-manager.ts:50-70`) supports resume, but `processor.ts` never passes `resume: true`. On timeout, line 394 deletes all partial output.

**Changes:**
- `services/worker/src/processor.ts`:
  - On job pickup, check if temp dir already has partial output → pass `resume: true` to `crawlSite()`
  - On `CrawlTimeoutError` catch (line 403): instead of deleting output, run the upload + completion flow for whatever pages finished. Set status to `timed_out` (new status) with partial counts.
  - Add `timed_out` to the DB schema status enum if not present
- `packages/scraper/src/crawler.ts`:
  - Already handles `resume` flag correctly via `filterUrlsForResume` — no changes needed

**Files:** `services/worker/src/processor.ts`, possibly `apps/api/src/db/schema.ts` (add `timed_out` status)

### B2 — Shared Work Queue (Replace Static Batching)

**Problem:** `crawler.ts:198-201` splits URLs into static per-browser queues via modulo. If one browser gets heavy pages, it bottlenecks while others idle.

**Changes:**
- `packages/scraper/src/crawler.ts`:
  - Replace `browserQueues` array with a single shared `AsyncIterator` or index counter
  - Each browser worker pulls the next URL from the shared pool when it finishes its current page
  - Use a single `let nextIndex = 0` at the crawl level (not per-browser) and atomic-like access via the event loop

**Files:** `packages/scraper/src/crawler.ts` (lines 198-258)

### B3 — Page-Level Retries with Exponential Backoff

**Problem:** `crawler.ts:243-249` logs failures and continues but never retries. Transient 503s or timeouts could succeed on retry.

**Changes:**
- `packages/scraper/src/crawler.ts`:
  - Wrap `processPage()` call (line 223) in a retry loop: max 2 retries, exponential backoff (2s, 4s)
  - Only retry on transient errors (timeout, 5xx). Don't retry on 4xx or abort signals.

**Files:** `packages/scraper/src/crawler.ts` (lines 222-250)

### B4 — Browser Context Reuse

**Problem:** `page-processor.ts:33-34` creates a new `browser.newContext()` + `context.newPage()` for every URL. Context creation is expensive (~100-200ms overhead).

**Changes:**
- `packages/scraper/src/page-processor.ts`:
  - Accept an optional `BrowserContext` param instead of `Browser`
  - If provided, create only a new `Page` within the existing context
  - Caller (crawler.ts) creates one context per browser and reuses it across all pages for that browser
- `packages/scraper/src/crawler.ts`:
  - Create `context = await browser.newContext()` once per browser instance
  - Pass context to `processPage()` instead of browser
  - Close context in the finally block

**Files:** `packages/scraper/src/page-processor.ts` (lines 20-34, 131-133), `packages/scraper/src/crawler.ts`

### B5 — Smarter Navigation Waits

**Problem:** `page-processor.ts:84-97` uses `networkidle` with 30s timeout + 2s extra wait. `networkidle` is unreliable on sites with persistent connections (analytics, WebSocket).

**Changes:**
- `packages/scraper/src/page-processor.ts`:
  - Switch primary wait to `domcontentloaded`
  - After DOM load, wait for key selectors that indicate content is ready (e.g., `body`, `main`, `[data-wf-page]` for Webflow)
  - Use `page.waitForLoadState('networkidle')` with a short 5s timeout as a *bonus* (don't fail on timeout)
  - Reduce the post-load wait from 2s to 500ms
  - Reduce post-scroll wait from 1s to 500ms

**Files:** `packages/scraper/src/page-processor.ts` (lines 84-97, 319-321)

### B6 — Cross-Crawl Asset Cache

**Problem:** `asset-downloader.ts:14` uses an in-memory `Map` cache that only lives for one crawl. Repeat crawls re-download identical assets (Webflow core JS/CSS, fonts, etc).

**Changes:**
- `packages/scraper/src/asset-cache.ts` — new file:
  - Content-addressable disk cache in a persistent directory (e.g., `/tmp/dxd-asset-cache` or configurable)
  - Key: SHA-256 of asset URL, Value: file on disk
  - `get(url)` → returns cached file path or null
  - `put(url, content)` → writes to cache
  - LRU eviction based on disk size limit (e.g., 2GB configurable)
- `packages/scraper/src/asset-downloader.ts`:
  - Accept optional `AssetCache` in constructor
  - Before `fetch()`, check cache → if hit, read from disk instead of network
  - After successful `fetch()`, write to cache
  - In-memory `Map` cache remains for intra-crawl dedup (faster than disk lookup)

**Files:** Create `packages/scraper/src/asset-cache.ts`, modify `packages/scraper/src/asset-downloader.ts`

### B7 — Link-Based URL Discovery (Spider Mode)

**Problem:** `sitemap-parser.ts` only reads `sitemap.xml`. Pages not in the sitemap are missed.

**Changes:**
- `packages/scraper/src/link-extractor.ts` — new file:
  - Given HTML content and page URL, extract all same-origin `<a href>` URLs
  - Normalize, deduplicate, filter by same hostname
- `packages/scraper/src/crawler.ts`:
  - Add `discoverLinks: boolean` option to `CrawlOptions`
  - After `processPage()` returns, run link extraction on the page's HTML
  - New URLs get added to the shared work queue (B2) for processing
  - Track visited URLs in a `Set` to prevent re-crawling
  - Cap total discovered URLs at `maxPages` to prevent runaway crawls

**Files:** Create `packages/scraper/src/link-extractor.ts`, modify `packages/scraper/src/crawler.ts`, `packages/scraper/src/types.ts`

---

## Implementation Order

```
B2 (shared work queue) — prerequisite for B7 (dynamic URL adding)
B4 (context reuse)     — independent
B5 (smarter waits)     — independent
B3 (page retries)      — independent
B1 (resume + partial)  — independent
B6 (asset cache)       — independent
B7 (link discovery)    — depends on B2

A0 (storage dual-runtime) — independent of B*
A1 (app factory + entry)  — depends on A0
A2 (route migration)      — depends on A1, sequential per route
A3 (infra provisioning)   — can start alongside A1
A4 (deploy)               — depends on A2 + A3
```

Scraper optimizations (B*) and API migration (A*) are fully independent tracks and can be worked in parallel.

## Verification

**Scraper optimizations:**
- Run a crawl against a known Webflow site before and after each optimization
- Compare: total time, pages succeeded/failed, output file count, output size
- Test resume: kill a crawl mid-way, restart, verify it picks up where it left off
- Test timeout: set `CRAWL_MAX_DURATION_MS=30000`, verify partial results are saved

**API migration:**
- `wrangler dev` locally — hit every endpoint, compare responses against Node API
- Auth flow: complete GitHub OAuth login → verify session persists
- SSE: start a crawl, open SSE stream, verify events arrive
- Preview: load an archived site, verify all assets render
- Zip download: download, unzip, verify contents
- Run existing test suite against both Node and Workers
