# Analysis: Playwright vs. cURL for Webflow Site Archiving

## Verdict: Playwright is the correct approach

A pure cURL/HTTP approach would produce **incomplete archives**. The current hybrid architecture (Playwright for discovery + fetch for downloads) is already the efficient pattern.

## Why Playwright is necessary

| Webflow Feature | Requires Browser? | Why |
|---|---|---|
| Lazy-loaded images (`data-src`, `data-bg`) | Yes | Triggered by scroll events |
| Webpack/Rspack dynamic chunks | Yes | Manifests exist only in JS runtime memory |
| Code Islands (federated React modules) | Yes | Module URLs resolved at execution time |
| Interaction-triggered content | Yes | Requires hover/click simulation |
| CSS `@import` chains & `url()` references | Partially | Network interception catches all; static parsing is fragile |

## What the current architecture already does right

- Playwright is used **only for discovery** (navigate, scroll, intercept network)
- Downloads use lightweight `fetch`, not Playwright (`asset-downloader.ts:110`)
- HTML rewriting uses Cheerio, not browser DOM
- Browser contexts are reused; concurrency scales to CPU/memory
- Assets are deduplicated in-memory and cached on disk

## Potential optimizations (if speed is a concern)

1. **Fast-path for static pages** — Try static HTML analysis first, fall back to Playwright only if dynamic content indicators are found (e.g., `data-wf-page`, chunk loaders)
2. **Increase page parallelism** — Run more concurrent pages per browser context
3. **Cross-crawl asset sharing** — Share the asset cache across crawls of the same site for incremental updates
4. **Sitemap-only mode** — For sites with complete sitemaps, skip link discovery to reduce processing

## No changes recommended

The current architecture is sound. No refactoring needed unless specific performance bottlenecks are identified.
